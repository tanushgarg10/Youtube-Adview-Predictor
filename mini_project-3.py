# -*- coding: utf-8 -*-
"""Mini_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AjYbtOm_SuIhZQHJvJSpNKMfuIIVXWvD

#Data Visualization
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("/content/train.csv.xls")

df.head()

df.shape

df.dtypes

category={'A': 1,'B': 2,'C': 3,'D': 4,'E': 5,'F': 6,'G': 7,'H': 8,}
df["category"]=df["category"].map(category)
df.head()

df=df[df.views!='F']
df=df[df.likes!='F']
df=df[df.dislikes!='F']
df=df[df.comment!='F']

df["views"]=pd.to_numeric(df["views"])
df["comment"]=pd.to_numeric(df["comment"])
df["likes"]=pd.to_numeric(df["likes"])
df["dislikes"]=pd.to_numeric(df["dislikes"])
df["adview"]=pd.to_numeric(df["adview"])

df.info()

column_vidid=df['vidid']

"""#Label Encoder"""

from sklearn.preprocessing import LabelEncoder

df.head()

import pandas as pd
import numpy as np

# Define the checki function
def checki(x):
  y = x[2:]
  h = ''
  m = ''
  s = ''
  mm = ''
  P = ['H','M','S']
  for i in y:
    if i not in P:
      mm+=i
    else:
      if(i=="H"):
        h = mm
        mm = ''
      elif(i == "M"):
        m = mm
        mm = ''
      else:
        s = mm
        mm = ''
  if(h==''):
    h = '00'
  if(m == ''):
    m = '00'
  if(s==''):
    s='00'
  bp = h+':'+m+':'+s
  return bp

# Convert the duration column to seconds
df['duration'] = df['duration'].apply(lambda x: checki(x).split(':')).apply(lambda x: int(x[0])*3600 + int(x[1])*60 + int(x[2]))
# Print the updated DataFrame
print(df.head())

#df['duration']=LabelEncoder().fit_transform(df['duration'])
df['vidid']=LabelEncoder().fit_transform(df['vidid'])
df['published']=LabelEncoder().fit_transform(df['published'])

df.head()

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


f, ax = plt.subplots(figsize=(10, 8))
corr = df.corr()
sns.heatmap(corr, mask=np.zeros_like(corr, dtype=bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax, annot=True)
plt.show()

df.info()

Y_train = pd.DataFrame(data = df.iloc[:, 1].values, columns = ['target'])
df=df.drop(["adview"],axis=1)
df=df.drop(["vidid"],axis=1)
df=df.drop(["published"],axis=1)
df.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df, Y_train, test_size=0.2, random_state=42)

X_train.shape

"""# MIN-MAX SCALER"""

# Normalise Data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.fit_transform(X_test)

X_train.mean()

# Evaluation Metrics
from sklearn import metrics
def print_error(X_test, y_test, model_name):
  prediction = model_name.predict(X_test)
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, prediction))
  print('Mean Squared Error:', metrics.mean_squared_error(y_test, prediction))
  print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))

"""#**Model** **Training**

# Linear Regression
"""

# Linear Regression
from sklearn import linear_model
linear_regression = linear_model.LinearRegression()
linear_regression.fit(X_train, y_train)
print_error(X_test,y_test, linear_regression)

"""# Support Vector Regressor"""

# Support Vector Regressor
from sklearn.svm import SVR
supportvector_regressor = SVR()
supportvector_regressor.fit(X_train,y_train)
print_error(X_test,y_test, supportvector_regressor)

"""# Descision Tree Regressor"""

# Decision Tree Regressor
from sklearn.tree import DecisionTreeRegressor
decision_tree = DecisionTreeRegressor()
decision_tree.fit(X_train, y_train)
print_error(X_test,y_test, decision_tree)

"""# Random Forest Regressor"""

# Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor
n_estimators = 200
max_depth = 25
min_samples_split=15
min_samples_leaf=2
random_forest = RandomForestRegressor(n_estimators = n_estimators, max_depth = max_depth, min_samples_split=min_samples_split)
random_forest.fit(X_train,y_train)
print_error(X_test,y_test, random_forest)

"""# ANN"""

# Artificial Neural Network
from tensorflow import keras
from keras.layers import Dense

ann = keras.models.Sequential([
                                Dense(6, activation="relu",
                                input_shape=X_train.shape[1:]),
                                Dense(6,activation="relu"),
                                Dense(1)
                                ])

optimizer=keras.optimizers.Adam()
loss=keras.losses.mean_squared_error
ann.compile(optimizer=optimizer,loss=loss,metrics=["mean_squared_error"])

history=ann.fit(X_train,y_train,epochs=100)

print_error(X_test,y_test,ann)

#Saving Scikitlearn models
import joblib
joblib.dump(supportvector_regressor, "SVR_youtubeadview.pkl")
# Saving Keras Artificial Neural Network model
ann.save("ann_youtubeadview.h5")

"""# the best model is random forest regressor using rmse value

#now testing
"""

dft = pd.read_csv("/content/test.csv.xls")

dft.head()

dft.shape

from keras.models import load_model
model = load_model("ann_youtubeadview.h5")

# Removing character "F" present in data
dft=dft[dft.views!='F']
dft=dft[dft.likes!='F']
dft=dft[dft.dislikes!='F']
dft=dft[dft.comment!='F']

dft.head()

dft.shape

# Assigning each category a number for Category feature
category={'A': 1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7,'H':8}
dft["category"]=dft["category"].map(category)
dft.head()

import pandas as pd
import numpy as np

# Define the checki function
def checki(x):
  y = x[2:]
  h = ''
  m = ''
  s = ''
  mm = ''
  P = ['H','M','S']
  for i in y:
    if i not in P:
      mm+=i
    else:
      if(i=="H"):
        h = mm
        mm = ''
      elif(i == "M"):
        m = mm
        mm = ''
      else:
        s = mm
        mm = ''
  if(h==''):
    h = '00'
  if(m == ''):
    m = '00'
  if(s==''):
    s='00'
  bp = h+':'+m+':'+s
  return bp

# Convert the duration column to seconds
dft['duration'] = dft['duration'].apply(lambda x: checki(x).split(':')).apply(lambda x: int(x[0])*3600 + int(x[1])*60 + int(x[2]))
# Print the updated DataFrame
print(df.head())

# Convert values to integers for views, likes, comments, dislikes and adview
dft["views"] = pd.to_numeric(dft["views"])
dft["comment"] = pd.to_numeric(dft["comment"])
dft["likes"] = pd.to_numeric(dft["likes"])
dft["dislikes"] = pd.to_numeric(dft["dislikes"])
column_vidid=dft['vidid']

# Endoding features like Category, Duration, Vidid
from sklearn.preprocessing import LabelEncoder
dft['vidid']=LabelEncoder().fit_transform(dft['vidid'])
dft['published']=LabelEncoder().fit_transform(dft['published'])
dft.head()

dft=dft.drop(["vidid"],axis=1)
dft=dft.drop(["published"],axis=1)
dft.head()

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_test = dft
X_test=scaler.fit_transform(X_test)

prediction = model.predict(X_test)

prediction=pd.DataFrame(prediction)
prediction.info()

prediction = prediction.rename(columns={0: "Adview"})

prediction.head()

prediction.to_csv('predictions.csv')

"""#pickle and json"""

import pickle
with open('yt_adview.pickle','wb') as f:
    pickle.dump(prediction,f)

import json

columns = {
    'data_columns' : [col.lower() for col in dft.columns]
}
with open("columns.json","w") as f:
    f.write(json.dumps(columns))

"""#random dataset"""

import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

# Define the new user-given data as a dictionary
new_data = {
    'views': 1110,
    'likes': 110,
    'dislikes': 611,
    'comment': 334,
    'duration': 1110,
    'category': 3,
}

# Convert the new data to a pandas DataFrame
new_df = pd.DataFrame([new_data])

# Convert the categorical variable to numerical using LabelEncoder
le = LabelEncoder()
new_df['category'] = le.fit_transform(new_df['category'])

# Normalize the numerical variables using MinMaxScaler
scaler = MinMaxScaler()
numeric_cols = ['views', 'likes', 'dislikes', 'duration','comment']
new_df[numeric_cols] = scaler.fit_transform(new_df[numeric_cols])

# Make the prediction using the random forest model
prediction = random_forest.predict(new_df)

# Print the prediction
print("Prediction:", prediction[0])